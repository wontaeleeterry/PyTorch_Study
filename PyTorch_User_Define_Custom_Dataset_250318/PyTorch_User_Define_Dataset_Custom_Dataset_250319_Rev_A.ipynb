{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사용자 정의 데이터셋(이미지 파일) & 커스텀 데이터셋\n",
    "\n",
    "- 딥러닝은 대량의 데이터를 이용하여 모델을 학습시킨다.\n",
    "- 데이터를 한번에 메모리에 불러와서 훈련시키면 시간과 비용 측면에서 비효울적이다.\n",
    "- 커스텀 데이터셋(custom dataset) : 데이터를 한 번에 다 부리지 않고 조금씩 나누어 불러서 사용하는 방식\n",
    "- 딥러닝 파이토치 교과서, 서지영 지음(p. 46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 파일을 이용하여 사용자 정의 데이터셋 만들기\n",
    "# 커스텀 데이터셋 클래스를 구현하기 위해 다음 3개의 함수를 구현한다.\n",
    "# __init__, __len__, __getitem__\n",
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "# import torch\n",
    "# from torchvision import datasets\n",
    "# from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_crop = transforms.CenterCrop((128, 128))   # 지정된 사이즈로 가운데 부분만 크롭\n",
    "# 사용하는 데이터셋의 이미지의 사이즈가 서로 달라, Dataload 부분에서 에러 발생\n",
    "# 커스텀 데이터셋 클래스의 메서드에서 전달하는 이미지를 미리 크롭하여 반환하도록 코드 설정(250318)\n",
    "\n",
    "class CustomImageDataset (Dataset):\n",
    "    # 라벨 정의 파일 csv, 이미지 저장 폴더\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file, names=['file_name', 'label'])   # 첫번째 컬럼 : 파일명 , 두번째 컬럼 : 라벨\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return transform_crop(image), label    # 이미지 텐서 + 라벨 반환\n",
    "        # 가운데 크롭된 이미지를 전달 (250318)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전달 변수명 일치시킬 것\n",
    "custom_img_dataset = CustomImageDataset(annotations_file='./img_labels/annotations_file.csv', img_dir='./img_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (tensor([[[205, 203, 200,  ..., 201, 200, 197],\n",
      "         [204, 202, 202,  ..., 201, 200, 196],\n",
      "         [202, 202, 200,  ..., 201, 200, 197],\n",
      "         ...,\n",
      "         [187, 188, 189,  ..., 195, 196, 196],\n",
      "         [187, 187, 188,  ..., 196, 197, 198],\n",
      "         [187, 187, 188,  ..., 197, 197, 199]],\n",
      "\n",
      "        [[186, 184, 181,  ..., 164, 164, 165],\n",
      "         [184, 182, 179,  ..., 164, 164, 166],\n",
      "         [182, 179, 177,  ..., 164, 164, 165],\n",
      "         ...,\n",
      "         [189, 188, 189,  ..., 180, 181, 178],\n",
      "         [189, 189, 190,  ..., 181, 182, 180],\n",
      "         [189, 189, 189,  ..., 182, 182, 182]],\n",
      "\n",
      "        [[154, 152, 148,  ..., 112, 114, 116],\n",
      "         [151, 147, 145,  ..., 112, 112, 114],\n",
      "         [147, 145, 143,  ..., 112, 112, 114],\n",
      "         ...,\n",
      "         [178, 178, 177,  ..., 151, 152, 154],\n",
      "         [178, 178, 177,  ..., 152, 153, 156],\n",
      "         [178, 176, 175,  ..., 153, 153, 156]]], dtype=torch.uint8), 'dog')\n",
      "1 (tensor([[[ 79,  79,  79,  ...,  67,  66,  66],\n",
      "         [ 78,  78,  78,  ...,  67,  66,  66],\n",
      "         [ 77,  77,  78,  ...,  66,  66,  66],\n",
      "         ...,\n",
      "         [154, 153, 152,  ..., 147, 152, 154],\n",
      "         [153, 152, 151,  ..., 146, 153, 155],\n",
      "         [152, 151, 151,  ..., 147, 155, 157]],\n",
      "\n",
      "        [[ 84,  84,  84,  ...,  71,  70,  70],\n",
      "         [ 83,  83,  83,  ...,  71,  70,  70],\n",
      "         [ 82,  82,  83,  ...,  70,  70,  70],\n",
      "         ...,\n",
      "         [167, 166, 165,  ..., 161, 166, 168],\n",
      "         [166, 165, 164,  ..., 161, 168, 170],\n",
      "         [165, 164, 164,  ..., 162, 170, 172]],\n",
      "\n",
      "        [[ 52,  52,  52,  ...,  46,  45,  45],\n",
      "         [ 51,  51,  51,  ...,  46,  45,  45],\n",
      "         [ 50,  50,  51,  ...,  45,  45,  45],\n",
      "         ...,\n",
      "         [ 87,  86,  85,  ...,  73,  78,  80],\n",
      "         [ 84,  83,  84,  ...,  70,  77,  79],\n",
      "         [ 83,  82,  84,  ...,  69,  79,  81]]], dtype=torch.uint8), 'dog')\n",
      "2 (tensor([[[  2,   2,   2,  ...,   1,   1,   1],\n",
      "         [  2,   2,   2,  ...,   1,   1,   1],\n",
      "         [  2,   2,   2,  ...,   1,   1,   1],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   3,   1,   1],\n",
      "         [  0,   0,   0,  ...,   2,   1,   1],\n",
      "         [  0,   0,   0,  ...,   2,   0,   0]],\n",
      "\n",
      "        [[106, 106, 106,  ..., 104, 104, 104],\n",
      "         [106, 106, 106,  ..., 104, 104, 104],\n",
      "         [106, 106, 106,  ..., 104, 104, 104],\n",
      "         ...,\n",
      "         [ 52,  52,  52,  ...,  62,  63,  63],\n",
      "         [ 53,  53,  53,  ...,  61,  63,  63],\n",
      "         [ 54,  54,  54,  ...,  61,  62,  62]],\n",
      "\n",
      "        [[217, 217, 217,  ..., 209, 209, 209],\n",
      "         [217, 217, 217,  ..., 209, 209, 209],\n",
      "         [217, 217, 217,  ..., 209, 209, 209],\n",
      "         ...,\n",
      "         [158, 158, 158,  ..., 166, 166, 166],\n",
      "         [161, 161, 161,  ..., 165, 166, 166],\n",
      "         [162, 162, 162,  ..., 165, 165, 165]]], dtype=torch.uint8), 'cat')\n",
      "3 (tensor([[[125, 122, 115,  ..., 133, 133, 133],\n",
      "         [126, 123, 114,  ..., 134, 134, 134],\n",
      "         [126, 123, 114,  ..., 136, 134, 134],\n",
      "         ...,\n",
      "         [102, 106, 108,  ..., 185, 184, 184],\n",
      "         [102, 105, 108,  ..., 185, 184, 184],\n",
      "         [101, 104, 109,  ..., 185, 184, 184]],\n",
      "\n",
      "        [[113, 112, 106,  ..., 118, 118, 118],\n",
      "         [114, 113, 105,  ..., 119, 119, 119],\n",
      "         [115, 113, 105,  ..., 118, 119, 119],\n",
      "         ...,\n",
      "         [ 95,  97,  97,  ..., 156, 157, 157],\n",
      "         [ 95,  96,  98,  ..., 156, 157, 157],\n",
      "         [ 94,  95,  99,  ..., 156, 157, 157]],\n",
      "\n",
      "        [[113, 113, 109,  ..., 115, 115, 115],\n",
      "         [114, 114, 108,  ..., 116, 116, 116],\n",
      "         [113, 112, 108,  ..., 114, 116, 116],\n",
      "         ...,\n",
      "         [102, 102, 101,  ..., 138, 140, 140],\n",
      "         [102, 101,  99,  ..., 138, 140, 140],\n",
      "         [101, 100, 100,  ..., 138, 140, 140]]], dtype=torch.uint8), 'cat')\n",
      "4 (tensor([[[208, 209, 210,  ..., 200, 197, 200],\n",
      "         [209, 210, 211,  ..., 198, 195, 198],\n",
      "         [209, 210, 211,  ..., 197, 193, 198],\n",
      "         ...,\n",
      "         [174, 206, 160,  ..., 191, 203, 197],\n",
      "         [194, 221, 185,  ..., 146, 181, 185],\n",
      "         [188, 187, 184,  ..., 135, 139, 169]],\n",
      "\n",
      "        [[192, 193, 194,  ..., 184, 181, 184],\n",
      "         [193, 194, 195,  ..., 182, 179, 182],\n",
      "         [193, 194, 195,  ..., 181, 177, 182],\n",
      "         ...,\n",
      "         [157, 189, 143,  ..., 181, 191, 187],\n",
      "         [176, 203, 167,  ..., 134, 167, 173],\n",
      "         [170, 169, 166,  ..., 122, 123, 156]],\n",
      "\n",
      "        [[179, 180, 181,  ..., 171, 168, 171],\n",
      "         [180, 181, 182,  ..., 169, 166, 169],\n",
      "         [180, 181, 182,  ..., 168, 164, 169],\n",
      "         ...,\n",
      "         [149, 179, 135,  ..., 169, 179, 175],\n",
      "         [166, 191, 157,  ..., 120, 154, 159],\n",
      "         [158, 155, 154,  ..., 106, 110, 140]]], dtype=torch.uint8), 'lion')\n"
     ]
    }
   ],
   "source": [
    "# 텐서 출력\n",
    "# enumerate 메서드 : 갯수 + 추출 데이터 반환\n",
    "\n",
    "for i, sample in enumerate(custom_img_dataset):\n",
    "    print(i, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[208, 209, 210,  ..., 200, 197, 200],\n",
       "         [209, 210, 211,  ..., 198, 195, 198],\n",
       "         [209, 210, 211,  ..., 197, 193, 198],\n",
       "         ...,\n",
       "         [174, 206, 160,  ..., 191, 203, 197],\n",
       "         [194, 221, 185,  ..., 146, 181, 185],\n",
       "         [188, 187, 184,  ..., 135, 139, 169]],\n",
       "\n",
       "        [[192, 193, 194,  ..., 184, 181, 184],\n",
       "         [193, 194, 195,  ..., 182, 179, 182],\n",
       "         [193, 194, 195,  ..., 181, 177, 182],\n",
       "         ...,\n",
       "         [157, 189, 143,  ..., 181, 191, 187],\n",
       "         [176, 203, 167,  ..., 134, 167, 173],\n",
       "         [170, 169, 166,  ..., 122, 123, 156]],\n",
       "\n",
       "        [[179, 180, 181,  ..., 171, 168, 171],\n",
       "         [180, 181, 182,  ..., 169, 166, 169],\n",
       "         [180, 181, 182,  ..., 168, 164, 169],\n",
       "         ...,\n",
       "         [149, 179, 135,  ..., 169, 179, 175],\n",
       "         [166, 191, 157,  ..., 120, 154, 159],\n",
       "         [158, 155, 154,  ..., 106, 110, 140]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서 -> 실제 이미지 변환\n",
    "tf_img = transforms.ToPILImage()\n",
    "img = tf_img(sample[0])\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지와 라벨 표시\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(custom_img_dataset, batch_size=3, shuffle=True)\n",
    "\n",
    "train_feature, train_label = next(iter(train_dataloader))\n",
    "\n",
    "img = train_feature[1].squeeze()\n",
    "label = train_label[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: dog\n"
     ]
    }
   ],
   "source": [
    "tf_img = transforms.ToPILImage()\n",
    "img = tf_img(img)\n",
    "img.show()\n",
    "\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치 이미지 처리: https://hands-on.pytorch.kr/object-detection/torchvision-basic-transforms.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
