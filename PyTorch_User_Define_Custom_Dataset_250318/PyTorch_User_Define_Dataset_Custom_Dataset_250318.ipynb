{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사용자 정의 데이터셋(이미지 파일) & 커스텀 데이터셋\n",
    "\n",
    "- 딥러닝은 대량의 데이터를 이용하여 모델을 학습시킨다.\n",
    "- 데이터를 한번에 메모리에 불러와서 훈련시키면 시간과 비용 측면에서 비효울적이다.\n",
    "- 커스텀 데이터셋(custom dataset) : 데이터를 한 번에 다 부리지 않고 조금씩 나누어 불러서 사용하는 방식\n",
    "- 딥러닝 파이토치 교과서, 서지영 지음(p. 46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 파일을 이용하여 사용자 정의 데이터셋 만들기\n",
    "# 커스텀 데이터셋 클래스를 구현하기 위해 다음 3개의 함수를 구현한다.\n",
    "# __init__, __len__, __getitem__\n",
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "# import torch\n",
    "# from torchvision import datasets\n",
    "# from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_crop = transforms.CenterCrop((64, 64))   # 지정된 사이즈로 가운데 부분만 크롭\n",
    "# 사용하는 데이터셋의 이미지의 사이즈가 서로 달라, Dataload 부분에서 에러 발생\n",
    "# 커스텀 데이터셋 클래스의 메서드에서 전달하는 이미지를 미리 크롭하여 반환하도록 코드 설정(250318)\n",
    "\n",
    "class CustomImageDataset (Dataset):\n",
    "    # 라벨 정의 파일 csv, 이미지 저장 폴더\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file, names=['file_name', 'label'])   # 첫번째 컬럼 : 파일명 , 두번째 컬럼 : 라벨\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return transform_crop(image), label    # 이미지 텐서 + 라벨 반환\n",
    "        # 가운데 크롭된 이미지를 전달 (250318)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전달 변수명 일치시킬 것\n",
    "custom_img_dataset = CustomImageDataset(annotations_file='./img_labels/annotations_file.csv', img_dir='./img_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (tensor([[[188, 164, 175,  ..., 186, 181, 201],\n",
      "         [190, 175, 176,  ..., 185, 198, 193],\n",
      "         [194, 187, 180,  ..., 195, 199, 194],\n",
      "         ...,\n",
      "         [171, 167, 146,  ..., 186, 201, 208],\n",
      "         [168, 149, 163,  ..., 189, 177, 192],\n",
      "         [152, 154, 159,  ..., 190, 151, 184]],\n",
      "\n",
      "        [[159, 135, 143,  ..., 156, 151, 169],\n",
      "         [162, 145, 147,  ..., 158, 170, 161],\n",
      "         [164, 157, 150,  ..., 167, 171, 162],\n",
      "         ...,\n",
      "         [155, 152, 131,  ..., 167, 182, 192],\n",
      "         [152, 134, 148,  ..., 170, 158, 176],\n",
      "         [136, 139, 144,  ..., 171, 135, 168]],\n",
      "\n",
      "        [[117,  95, 102,  ..., 104, 101, 122],\n",
      "         [123, 107, 107,  ..., 105, 120, 114],\n",
      "         [128, 121, 112,  ..., 117, 121, 115],\n",
      "         ...,\n",
      "         [122, 121, 100,  ..., 135, 150, 158],\n",
      "         [119, 103, 117,  ..., 138, 126, 142],\n",
      "         [103, 108, 113,  ..., 141, 102, 135]]], dtype=torch.uint8), 'dog')\n",
      "1 (tensor([[[215, 215, 211,  ..., 253, 244, 232],\n",
      "         [225, 238, 246,  ..., 255, 248, 235],\n",
      "         [244, 247, 247,  ..., 254, 236, 226],\n",
      "         ...,\n",
      "         [231, 227, 225,  ..., 158, 157, 145],\n",
      "         [228, 226, 229,  ..., 155, 148, 132],\n",
      "         [228, 227, 233,  ..., 152, 153, 139]],\n",
      "\n",
      "        [[206, 206, 202,  ..., 247, 236, 224],\n",
      "         [216, 229, 237,  ..., 251, 240, 227],\n",
      "         [235, 238, 238,  ..., 248, 228, 218],\n",
      "         ...,\n",
      "         [225, 221, 216,  ..., 158, 157, 145],\n",
      "         [222, 220, 220,  ..., 155, 148, 132],\n",
      "         [222, 221, 224,  ..., 152, 153, 139]],\n",
      "\n",
      "        [[207, 207, 203,  ..., 247, 234, 222],\n",
      "         [217, 230, 238,  ..., 251, 238, 225],\n",
      "         [236, 239, 239,  ..., 248, 226, 216],\n",
      "         ...,\n",
      "         [225, 221, 219,  ..., 148, 147, 135],\n",
      "         [222, 220, 223,  ..., 143, 136, 122],\n",
      "         [224, 221, 229,  ..., 140, 141, 127]]], dtype=torch.uint8), 'dog')\n",
      "2 (tensor([[[ 38,  52,  61,  ..., 152, 149, 148],\n",
      "         [ 65,  80,  98,  ..., 170, 161, 161],\n",
      "         [ 92, 110, 134,  ..., 194, 178, 178],\n",
      "         ...,\n",
      "         [182, 192, 201,  ...,   8,   1,   0],\n",
      "         [174, 187, 196,  ...,   5,   0,   0],\n",
      "         [166, 181, 192,  ...,  10,   0,   0]],\n",
      "\n",
      "        [[112, 118, 120,  ..., 128, 121, 122],\n",
      "         [122, 129, 136,  ..., 146, 132, 133],\n",
      "         [136, 141, 155,  ..., 170, 148, 147],\n",
      "         ...,\n",
      "         [184, 195, 204,  ...,  84,  83,  83],\n",
      "         [176, 189, 199,  ...,  84,  83,  82],\n",
      "         [168, 183, 194,  ...,  93,  82,  82]],\n",
      "\n",
      "        [[201, 202, 196,  ..., 124, 117, 121],\n",
      "         [199, 198, 198,  ..., 142, 126, 130],\n",
      "         [201, 196, 202,  ..., 166, 140, 142],\n",
      "         ...,\n",
      "         [205, 214, 221,  ..., 180, 182, 185],\n",
      "         [199, 210, 218,  ..., 185, 185, 184],\n",
      "         [193, 206, 215,  ..., 197, 186, 186]]], dtype=torch.uint8), 'cat')\n",
      "3 (tensor([[[ 87,  87,  78,  ..., 150, 166, 172],\n",
      "         [ 93,  90,  83,  ..., 149, 151, 131],\n",
      "         [ 87,  83,  70,  ..., 155, 151, 119],\n",
      "         ...,\n",
      "         [ 79, 106,  96,  ..., 163, 172, 179],\n",
      "         [ 76,  99,  95,  ..., 167, 169, 173],\n",
      "         [ 76,  93,  87,  ..., 194, 187, 181]],\n",
      "\n",
      "        [[ 78,  78,  69,  ..., 149, 165, 171],\n",
      "         [ 84,  81,  74,  ..., 148, 150, 130],\n",
      "         [ 78,  74,  61,  ..., 154, 150, 118],\n",
      "         ...,\n",
      "         [ 76, 103,  95,  ..., 160, 169, 176],\n",
      "         [ 73,  96,  94,  ..., 166, 168, 172],\n",
      "         [ 73,  90,  86,  ..., 193, 186, 180]],\n",
      "\n",
      "        [[ 83,  83,  74,  ..., 157, 173, 179],\n",
      "         [ 89,  86,  79,  ..., 156, 158, 138],\n",
      "         [ 83,  79,  66,  ..., 162, 158, 126],\n",
      "         ...,\n",
      "         [ 83, 110, 100,  ..., 155, 164, 171],\n",
      "         [ 80, 103,  99,  ..., 161, 163, 167],\n",
      "         [ 80,  97,  91,  ..., 188, 181, 175]]], dtype=torch.uint8), 'cat')\n",
      "4 (tensor([[[174, 179, 203,  ...,  68,  57,  60],\n",
      "         [179, 179, 218,  ...,  69,  53,  62],\n",
      "         [208, 213, 227,  ...,  69,  56,  61],\n",
      "         ...,\n",
      "         [190, 187, 186,  ...,  81, 158, 172],\n",
      "         [186, 195, 206,  ...,  60, 178, 193],\n",
      "         [205, 211, 218,  ...,  42, 169, 206]],\n",
      "\n",
      "        [[136, 142, 170,  ...,  67,  56,  59],\n",
      "         [141, 142, 185,  ...,  68,  52,  61],\n",
      "         [171, 179, 195,  ...,  68,  57,  62],\n",
      "         ...,\n",
      "         [176, 173, 172,  ...,  73, 151, 165],\n",
      "         [172, 181, 192,  ...,  51, 170, 186],\n",
      "         [191, 197, 204,  ...,  33, 160, 198]],\n",
      "\n",
      "        [[ 89,  97, 127,  ...,  73,  62,  65],\n",
      "         [ 96,  97, 142,  ...,  74,  58,  67],\n",
      "         [126, 134, 154,  ...,  73,  62,  67],\n",
      "         ...,\n",
      "         [147, 144, 145,  ...,  62, 135, 147],\n",
      "         [143, 152, 163,  ...,  42, 159, 170],\n",
      "         [162, 168, 175,  ...,  26, 151, 185]]], dtype=torch.uint8), 'lion')\n"
     ]
    }
   ],
   "source": [
    "# 텐서 출력\n",
    "# enumerate 메서드 : 갯수 + 추출 데이터 반환\n",
    "\n",
    "for i, sample in enumerate(custom_img_dataset):\n",
    "    print(i, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# 텐서 -> 실제 이미지 변환\\ntf_img = transforms.ToPILImage()\\nimg = tf_img(sample[0])\\nimg.show()\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# 텐서 -> 실제 이미지 변환\n",
    "tf_img = transforms.ToPILImage()\n",
    "img = tf_img(sample[0])\n",
    "img.show()\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지와 라벨 표시\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# train_dataloader = DataLoader(custom_img_dataset, batch_size=16, shuffle=True)\n",
    "train_dataloader = DataLoader(custom_img_dataset, batch_size=3, shuffle=True)\n",
    "\n",
    "# data_iter = iter(train_dataloader)\n",
    "# train_feature, train_label = data_iter.next()\n",
    "train_feature, train_label = next(iter(train_dataloader))\n",
    "\n",
    "# print(f\"Feature batch shape: {train_feature.size()}\")\n",
    "# print(f\"Labels batch shape: {train_label.size()}\")\n",
    "\n",
    "img = train_feature[1].squeeze()\n",
    "\n",
    "# squeeze() 메서드의 기능 : [[1, 2, 3, 4]]를 [1, 2, 3, 4]의 형태도 변경\n",
    "label = train_label[1]\n",
    "\n",
    "# plt.imshow(img, cmap=\"gray\")\n",
    "# height x width x channel 순으로 저장되어야 이미지 출력 가능\n",
    "\n",
    "plt.imshow(img.permute(1, 2, 0))   # 채널의 순서를 임의로 조정한다.\n",
    "plt.show()\n",
    "\n",
    "print(f\"Label: {label}\")\n",
    "\n",
    "# 에러 문제 : 해당 에러는 사용하는 데이터셋의 이미지의 사이즈가 서로 달라서입니다. \n",
    "# 사이즈가 다르게되면 Array나 Tensor의 각 차원이 동일하지 않기 때문에 batch형태로 묶어줄 수 없기 때문에 발생합니다.\n",
    "# https://cchhoo407.tistory.com/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치 이미지 처리: https://hands-on.pytorch.kr/object-detection/torchvision-basic-transforms.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
